{
  "intro": [
    {
      "type": "markdown",
      "value": "Language is a human skill. Computers were historically very bad at language...until LLMs came along."
    }
  ],
  "title": "Tokenization",
    "firstSection": [
    {
      "type": "markdown",
      "value": "For humans, language is made up of words. But an LLM has it’s own kind of word, called a “token”. Humans read words by considering what type of word it is (noun, verb, etc.) and what it means."
    },
    {
      "type": "markdown",
      "value": "For example, we might see this sentence and group each word into a part of speech, which tells us something about the word’s meaning."
    },
    {
      "type": "markdown",
      "value": "But LLMs do not understand language this way. They do not group words by their meaning, or by their part of speech."
    },
    {
      "type": "markdown",
      "value": "Instead, they break words down into smaller pieces called tokens. These tokens are then grouped together in a sequence to form a sentence."
    }
  ],
  "secondSection": [
    {
      "type": "markdown",
      "value": "Many people think of LLMs as a sort of “black box”: you ask a question, and an answer magically appears! But there are ways to understand these models—and starting with tokens can help."
    },
    {
      "type": "markdown",
      "value": "A token can be a single word, as shown in the diagram here. This is similar to how humans think of 'words'."
    },
    {
      "type": "markdown",
      "value": "Or, tokens can be a series of words. If we increase the length of our tokens, then the number of tokens we need to represent a sentence decreases. But there's also a trade-off: the longer the token, the less flexibility we have in understanding the meaning of the sentence. And the larger our model becomes."
    },
    {
      "type": "markdown",
      "value": "To increase efficiency, we could also break down words into smaller parts, like 'cat' into 'c', 'a', and 't'. This presents another tradeoff: the smaller the token, the more tokens we need to represent a sentence, but the more flexibility we have in understanding the meaning of the sentence."
    },
    {
      "type": "markdown",
      "value": "How does a model make connections between words? How does it understand something?"
    }
  ],
  "thirdSection": [
    {
      "type": "markdown",
      "value": "We can examine the layers of the model to see how it processes tokens. In this graph, each dot is a sentence with the word 'bank' in it, grouped by similarity. Here is one layer of the model."
    },
    {
      "type": "markdown",
      "value": "As we progress through the layers, we can see how the model's understanding of the word 'bank' evolves."
    },
    {
      "type": "markdown",
      "value": "In this way"
    },
    {
      "type": "markdown",
      "value": "We might understand"
    },
    {
      "type": "markdown",
      "value": "How the LLM 'understands' the word 'bank'."
    }
  ]
}